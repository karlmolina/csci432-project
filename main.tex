\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{algorithm2e}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}


\title{Group 11 P3}
\author{Arnold Smithson, Karl Molina, Kyle Webster}
\date{November 2019}

\begin{document}

\maketitle

\section{Introduction}

Suppose an organization such as Amazon wants to group customers based on their purchase history to recommend other products to a given customer. The organization attempts to take a customer's purchase history and compare it to other customers with similar purchases. This is an example of where a DBSCAN algorithm would be useful. DBSCAN, otherwise expanded as Density-Based Spatial Clustering of Applications with Noise, is a data-clustering algorithm meant to solve the general problem of "How are these data points related?" For the customers examined above, they could be clustered based on items that they both bought. However, DBSCAN has many other applications.

\section{Background}

The inception of the first DBSCAN algorithm began in 1996 when Ester used the newly-created algorithm to sort a large number of data points despite the presence of noise in databases \cite{ester1996density}. To do this, the algorithm employs a "nearest-neighbor"-esque line of thinking. It classifies each point as either a boarder point, a core point, or noise by determining the number of points within a spherical radius of it. The number of points within a spherical radius is a tunable parameter which determines the classification of the points. For example, if the number of points required is 5, then a core point is a point with 5 points within it's spherical radius. A border point is a point that does not have the required number of points to become a core point, but one of the points in it's sphere is a core point. A noise point is one that fails both the numbers and the check to see if the points within it's sphere is a core point. The clustering groups the core points to other core and border points until distinct clusters are created and no core points remain. Using the example above, we can take a customer and group them based on their purchase compared to other customers. If they bought a cat toy and a cat bed and other customers purchased a cat toy, cat bed, and cat litter, then the algorithm will group them together for the same recommendations of cat food. 

\section{Applications}
Clustering of data points is an unsupervised machine learning method, meaning that you may not know exactly what the clusters may represent. DBSCAN has advantages over other clustering methods because by the nature of the algorithm the clusters do not need to be convex. A clustering method such as K-Means defines clusters by the closest points by distance to a centroid point. This means that the clusters will be spherical in shape. DBSCAN uses every core point to find a cluster which means that it can result in non-convex clusters. By understanding the algorithm we know that it inherently will deal with noise in the given data set. Algorithms like K-Means will put each point in a cluster and has no way of describing what points are considered noise.

A downfall of DBSCAN is that it can increase greatly in time complexity with the amount of features that a data point has \cite{Gan:2015:DRM:2723372.2737792}. Gan and Tao \cite{Gan:2015:DRM:2723372.2737792} show that DBSCAN can be infeasible for a feature dimension greater than 2 in which they suggest an altered approximate DBSCAN algorithm. However, the dimensionality constraint can be combated with feature selection. In summary, we find that we can apply DBSCAN to clustering problems where the clusters may not be convex. We can also apply it to noisy data. However, we must be careful when using data sets with a high number of features.

\section{Pseudocode}
The pseudocode in this section is taken from \cite{gunawan2013faster}. The function $RangeQuery(p, \epsilon)$ finds all points in the data set $D$ that are less than $\epsilon$ distance away from point $p$. This algorithm loops through all the points in a data set and finds the core points. The core points have $minPts$ number of points in a hyper-sphere around them of distance $\epsilon$. It then assigns a core point to a cluster and expands the cluster to include any neighboring points. When its finished, all clusters will consist of neighboring core points and other neighboring points. Any other point will be labeled as noise.

\begin{algorithm}[H]
    // $D$ is a set of unclassified points\;
    
    // $\epsilon$ is the maximum distance
    
    // $minPts$ is the minimum \# of points to form a cluster
    
    Initialize cluster id C = 0\;
    
    \For{each unclassified point $p \in D$}{
        $N_\epsilon(p) \gets RangeQuery(p,\epsilon)$
        
        \eIf{$|N_\epsilon(p)| \geq minPts$}{
            Set $p$'s cluster id to $C$
            
            $ExpandCluster(p, N_\epsilon(p), C, \epsilon, minPts)$
            
            $C \gets C + 1$
        }{
            Label p as noise
        }
    }
 \caption{DBSCAN($D$, $\epsilon$, $minPts$)}
\end{algorithm}

\begin{algorithm}[H]
    
    \For{each point $q \in NeighborPts$}{
        \If{q is unclassified}{
            $N_\epsilon(p) \gets RangeQuery(p,\epsilon)$
            
            \If{$|N_\epsilon(p)| \geq minPts$}{
                $NeighborPts \gets NeighborPts \cup N_\epsilon(p)$
            }
        }
        \If{q does not belong to any cluster}{
            Set $q$'s cluster id to C
        }
    }
 \caption{ExpandCluster($p$, $NeighborPts$, $C$, $\epsilon$, $minPts$)}
\end{algorithm}

\newpage
\bibliography{bibliography}
\bibliographystyle{ieeetr}


\end{document}
