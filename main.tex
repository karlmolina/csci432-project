\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{algorithm2e}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}


\title{Group 11 Advanced Algorithms Project}
\author{Arnold Smithson, Karl Molina, Kyle Webster}
\date{November 2019}

\begin{document}

\maketitle

\section{Introduction}

Suppose an organization such as Amazon wants to group customers based on their purchase history to recommend other products to a given customer. The organization attempts to take a customer's purchase history and compare it to other customers with similar purchases. This is an example of where a DBSCAN algorithm would be useful. DBSCAN, otherwise expanded as Density-Based Spatial Clustering of Applications with Noise, is a data-clustering algorithm meant to solve the general problem of "How are these data points related?" For the customers examined above, they could be clustered based on items that they both bought. However, DBSCAN has many other applications.

\section{Background}

The inception of the first DBSCAN algorithm began in 1996 when Ester used the newly-created algorithm to sort a large number of data points despite the presence of noise in databases \cite{ester1996density}. To do this, the algorithm employs a "nearest-neighbor"-esque line of thinking. It classifies each point as either a boarder point, a core point, or noise by determining the number of points within a spherical radius of it. The number of points within a spherical radius is a tunable parameter which determines the classification of the points. For example, if the number of points required is 5, then a core point is a point with 5 points within it's spherical radius. A border point is a point that does not have the required number of points to become a core point, but one of the points in it's sphere is a core point. A noise point is one that fails both the numbers and the check to see if the points within it's sphere is a core point. The clustering groups the core points to other core and border points until distinct clusters are created and no core points remain. Using the example above, we can take a customer and group them based on their purchase compared to other customers. If they bought a cat toy and a cat bed and other customers purchased a cat toy, cat bed, and cat litter, then the algorithm will group them together for the same recommendations of cat food. 

\section{Applications}
Clustering of data points is an unsupervised machine learning method, meaning that you may not know exactly what the clusters may represent. DBSCAN has advantages over other clustering methods because by the nature of the algorithm the clusters do not need to be convex. A clustering method such as K-Means defines clusters by the closest points by distance to a centroid point. This means that the clusters will be spherical in shape. DBSCAN uses every core point to find a cluster which means that it can result in non-convex clusters. By understanding the algorithm we know that it inherently will deal with noise in the given data set. Algorithms like K-Means will put each point in a cluster and has no way of describing what points are considered noise.

A downfall of DBSCAN is that it can increase greatly in time complexity with the amount of features that a data point has \cite{Gan:2015:DRM:2723372.2737792}. Gan and Tao \cite{Gan:2015:DRM:2723372.2737792} show that DBSCAN can be infeasible for a feature dimension greater than 2 in which they suggest an altered approximate DBSCAN algorithm. However, the dimensionality constraint can be combated with feature selection. In summary, we find that we can apply DBSCAN to clustering problems where the clusters may not be convex. We can also apply it to noisy data. However, we must be careful when using data sets with a high number of features.

\section{Pseudocode}
The pseudocode in this section is taken from \cite{gunawan2013faster}. The function $RangeQuery(p, \epsilon)$ finds all points in the data set $D$ that are less than $\epsilon$ distance away from point $p$. This algorithm loops through all the points in a data set and finds the core points. The core points have $minPts$ number of points in a hyper-sphere around them of distance $\epsilon$. It then assigns a core point to a cluster and expands the cluster to include any neighboring points. When its finished, all clusters will consist of neighboring core points and other neighboring points. Any other point will be labeled as noise.

\begin{algorithm}[H]
    // $D$ is a set of unclassified points\;
    
    // $\epsilon$ is the maximum distance
    
    // $minPts$ is the minimum \# of points to form a cluster
    
    Initialize cluster id C = 0\;
    
    \For{each unclassified point $p \in D$}{
        $N_\epsilon(p) \gets RangeQuery(p,\epsilon)$
        
        \eIf{$|N_\epsilon(p)| \geq minPts$}{
            Set $p$'s cluster id to $C$
            
            $ExpandCluster(p, N_\epsilon(p), C, \epsilon, minPts)$
            
            $C \gets C + 1$
        }{
            Label p as noise
        }
    }
 \caption{DBSCAN($D$, $\epsilon$, $minPts$)}
\end{algorithm}

\begin{algorithm}[H]
    
    \For{each point $q \in NeighborPts$}{
        \If{q is unclassified}{
            $N_\epsilon(p) \gets RangeQuery(p,\epsilon)$
            
            \If{$|N_\epsilon(p)| \geq minPts$}{
                $NeighborPts \gets NeighborPts \cup N_\epsilon(p)$
            }
        }
        \If{q does not belong to any cluster}{
            Set $q$'s cluster id to C
        }
    }
 \caption{ExpandCluster($p$, $NeighborPts$, $C$, $\epsilon$, $minPts$)}
\end{algorithm}
\newpage
\section{Plus One}
This algorithm has a heavy background in machine learning, a topic some people might not explore while they earn their Computer Science degree or in a career in general. There are some examples, but a lot of them aren't clearly explained or aren't very transparent in every step. Thus, for the plus one element of our project, we plan to show how DBSCAN works using a graphical presentation and visually compare the results with another clustering algorithm known as K-Means Clustering. By using the p5 python package to plot points in an animation-like style, we are going to show how different parameters of DBSCAN, such as $eps$ and $minPts$, affect the resulting clusters. When we are running K-Means, we will use the same number of clusters that DBSCAN found. For simplicity of visualization, we will use a two feature data set so that we can plot the data points in a two-dimensional space. 

When first showing how the algorithm works, we will use a small data set to exemplify the circle of radius $eps$ around each point and how points are separated into core points, border points, and noise points by color-coding each type of point. On a large data set we will show the effects of changing the parameters of the algorithm $minPts$ and $eps$. To show an example of tuning the algorithm we will start with bad parameters and slowly improve them. Once we show the results of the tuned DBSCAN algorithm we will see that DBSCAN can find clusters that are not convex. To show the different clusters that were found we will use different symbols and colors. We will run the same data set with K-Means and show that K-Means clusters must be convex. This will show that DBSCAN will find more types of clusters than K-Means. We will also run a different data set with both algorithms to show how each algorithm deals with noise. For our data set we plan on creating our own contrived set that easily shows non convex clusters. Then, we will add random points to that data set to act as noise. 

\newpage
\bibliography{bibliography}
\bibliographystyle{ieeetr}


\end{document}
